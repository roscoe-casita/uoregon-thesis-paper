\chapter{Future Directions}

\section{Distributed Computation via Iterative Containers}
Fundamentally this paper re-introduces a polynomial space algorithm for computing hypergraph transversals as an iterative procedure. Each work item that is processed is a partial transversal with the included negative sets and a new hyperedge. The abstraction completely captures all the data needed to generate the next set of work items. The N-Way-Tree algorithm that iteratively constructs the next work items or processes completed transversals can be easily replaced.\\

The replacement for the N-Way-Tree would be a distributed processing dispatcher. Each work item would be put in a queue for execution on a compute node. All data to process the node is present in the work item. All the future work items can be generated on a compute node, then enqueued in the distributed dispatcher processor. \\

Currently hyperedges are always encountered in the same order at each depth of the tree. A pipeline of compute nodes with hyperedges can be built to distribute the work. A compute node could be paired with a specific hyperedge and a work queue. As each work item is created by the previous compute node it would be enqueued in the next compute node work queue.\\

Controlling work item generation to keep the pipeline full becomes the new problem. A cascading exponential number of work items generated by the early compute nodes will flood the later parts. The expansion of work items is currently exponential in the maximal worst case. Fundamentally another odometer state can be added to the partial transversal frame and used to iteratively generate the next set of work items. As the pipeline of work items becomes empty the work items would be iterated, generated, and added to the pipeline to keep it full until the current work item is processed.\\


\section{Hyperedge Visitation Manipulation}



An interesting and notable effect of encountering a new hyperedge, is when the partial transversal negates all new transversals, causing the complete removal of all future work items. Simply a new hyperedge is encountered that causes a minimal transversal to either become invalid or redundant. Eliminating future minimal transversals before they generate children that need to be eliminated is a possible optimization. \\

Reordering the visitation of hyperedges would need to be provably sound such that an dispatch algorithm could reorder the visitation of hyperedges to eliminate minimal transversals as quickly as possible during computation. As each work item has a list of negation odometers, it is possible to look for a hyperedge contained in the negation odometer union with the transversal odometer to generate either 1 new minimal transversal child or not being an appropriate minimal transversal and being eliminated from the work queue. \\






\section{Advanced Algorithmic Modifications}
The inter-section, outer-section, and cross product sections each generate a work item. Currently all work items for a given level are expanded to the next level (exponential in the worst case), then processing the first one in the same way expanding the tree in a depth first search. Control of the expansion can be done via additional odometer states in each work item. Replacement of the function $Gen2expNtruefalse$ with an iterative generator allows for a more controlled expansion. \\

The generation of the next list of work items is currently a fixed ordering that generates the same traversals in the same order every time. Obviously this is desirable from a completeness aspect, but from a practical perspective iterating the traversals which satisfy optimality constraints `better' is of particular interest. `Better' is a term that needs to be defined in terms of the particular problem that is being solved. \cite{khachiyan2006efficient}\\




\section{Experimental Research}
A possible direction of research is to modify the core algorithms to look for ways to collapse the work items being generated at each depth. The compact transversal representations generated by this algorithm store exponential traversals in polynomial space. Removal of the negation sets in conjunction with collapsing work items at each level would result in a polynomial reduction by storing traversals in an exponential encoding. This is the partial transversal frame that is enumerated to generate its transversals \\

It is not possible to enumerate all transversals of a hypergraph in polynomial space and polynomial time. It may be possible to enumerate all exponential encodings of transversals of a hypergraph in polynomial space and polynomial time. \\

The current algorithm \textit{already} uses a polynomial space encoding of an exponential transversal. The partial transversal frame is interpreted as an encoding that generates exponential transversals. A partial transversals can be merged with another partial transversal if and only if the following holds: They contain the same number of generalized variables. There is only one generalized variable in both transversals that are not equivalent. Notice that both collapsing and comparing against all transversals at a given level is polynomial in space and time. Collapsing all partial transversals at every depth would need to be proven to work correctly.\\

The output of such an algorithm would be a polynomial space encoding, the same as today. Extracting all of the transversals via interpretation of the encoding is an exponential time operation. Instead of extracting all the transversals, the encodings themselves could represent such things as the clique of cliques, central clusters in clusters, or the eigenvalues that are of the same size.\\

